{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE VACUUM WORLD   \n",
    "\n",
    "In this notebook, we will be discussing **the structure of agents** through an example of the **vacuum agent**. The job of AI is to design an **agent program** that implements the agent function: the mapping from percepts to actions. We assume this program will run on some sort of computing device with physical sensors and actuators: we call this the **architecture**:\n",
    "\n",
    "<h3 align=\"center\">agent = architecture + program</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, please review [<b>agents.ipynb</b>](https://github.com/aimacode/aima-python/blob/master/agents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENTS\n",
    "\n",
    "* Agent\n",
    "* Random Agent Program\n",
    "* Table-Driven Agent Program\n",
    "* Simple Reflex Agent Program\n",
    "* Model-Based Reflex Agent Program\n",
    "* Goal-Based Agent Program\n",
    "* Utility-Based Agent Program\n",
    "* Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENT PROGRAMS\n",
    "\n",
    "An agent program takes the current percept as input from the sensors and returns an action to the actuators. There is a difference between an agent program and an agent function: an agent program takes the current percept as input whereas an agent function takes the entire percept history.\n",
    "\n",
    "The agent program takes just the current percept as input because nothing more is available from the environment; if the agent's actions depend on the entire percept sequence, the agent will have to remember the percept.\n",
    "\n",
    "We'll discuss the following agent programs here with the help of the vacuum world example:\n",
    "\n",
    "* Random Agent Program\n",
    "* Table-Driven Agent Program\n",
    "* Simple Reflex Agent Program\n",
    "* Model-Based Reflex Agent Program\n",
    "* Goal-Based Agent Program\n",
    "* Utility-Based Agent Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Program\n",
    "\n",
    "A random agent program, as the name suggests, chooses an action at random, without taking into account the percepts.   \n",
    "Here, we will demonstrate a random vacuum agent for a trivial vacuum environment, that is, the two-state environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing all the functions from the agents module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/bach/l/under/seancobb/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "from agents import *\n",
    "from notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first see how we define the TrivialVacuumEnvironment. Run the next cell to see how abstract class TrivialVacuumEnvironment is defined in agents module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psource(TrivialVacuumEnvironment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the Environment: {(0, 0): 'Clean', (1, 0): 'Dirty', (0, 1): 'Dirty', (1, 1): 'Clean'}.\n"
     ]
    }
   ],
   "source": [
    "# These are the two locations for the 2x2 environment\n",
    "loc_A, loc_B, loc_C, loc_D = (0, 0), (1, 0), (0, 1), (1, 1)\n",
    "\n",
    "\n",
    "# Initialize the two-state environment\n",
    "trivial_vacuum_env = TrivialVacuumEnvironment()\n",
    "\n",
    "\n",
    "# Check the initial state of the environment\n",
    "print(\"State of the Environment: {}.\".format(trivial_vacuum_env.status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our agent now. This agent will choose any of the actions from 'Right', 'Left', 'Suck' and 'NoOp' (No Operation) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random agent\n",
    "random_agent = Agent(program=RandomAgentProgram(['Right', 'Left', 'Suck', 'NoOp', 'Down', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add our agent to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomVacuumAgent is located at (0, 1).\n"
     ]
    }
   ],
   "source": [
    "# Add agent to the environment\n",
    "trivial_vacuum_env.add_thing(random_agent)\n",
    "\n",
    "print(\"RandomVacuumAgent is located at {}.\".format(random_agent.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our environment now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the Environment: {(0, 0): 'Clean', (1, 0): 'Dirty', (0, 1): 'Dirty', (1, 1): 'Clean'}.\n",
      "RandomVacuumAgent is located at (0, 1).\n",
      "State of the Environment: {(0, 0): 'Clean', (1, 0): 'Dirty', (0, 1): 'Dirty', (1, 1): 'Clean'}.\n",
      "RandomVacuumAgent is located at (0, 2).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(0, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Run the environment\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrivial_vacuum_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Check the current state of the environment\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState of the Environment: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trivial_vacuum_env\u001b[38;5;241m.\u001b[39mstatus))\n",
      "File \u001b[0;32m~/cs440/aima-python/agents.py:341\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39malive:\n\u001b[0;32m--> 341\u001b[0m         actions\u001b[38;5;241m.\u001b[39mappend(agent\u001b[38;5;241m.\u001b[39mprogram(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpercept\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m         actions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/cs440/aima-python/agents.py:794\u001b[0m, in \u001b[0;36mTrivialVacuumEnvironment.percept\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpercept\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[1;32m    793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the agent's location, and the location status (Dirty/Clean).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mlocation, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 2)"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # Run the environment\n",
    "    trivial_vacuum_env.step()\n",
    "\n",
    "    # Check the current state of the environment\n",
    "    print(\"State of the Environment: {}.\".format(trivial_vacuum_env.status))\n",
    "\n",
    "    print(\"RandomVacuumAgent is located at {}.\".format(random_agent.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLE-DRIVEN AGENT PROGRAM\n",
    "\n",
    "A table-driven agent program keeps track of the percept sequence and then uses it to index into a table of actions to decide what to do. The table represents explicitly the agent function that the agent program embodies.  \n",
    "In the two-state vacuum world, the table would consist of all the possible states of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {((loc_A, 'Clean'),): 'Right',\n",
    "             ((loc_A, 'Dirty'),): 'Suck',\n",
    "             ((loc_B, 'Clean'),): 'Up',\n",
    "             ((loc_B, 'Dirty'),): 'Suck',\n",
    "             ((loc_C, 'Clean'),): 'Down',\n",
    "             ((loc_C, 'Dirty'),): 'Suck',\n",
    "             ((loc_D, 'Clean'),): 'Left',\n",
    "             ((loc_D, 'Dirty'),): 'Suck',\n",
    "             \n",
    "             ((loc_A, 'Dirty'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Dirty')): 'Suck',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_B, 'Dirty'), (loc_B, 'Clean')): 'Up',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_C, 'Dirty'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Dirty')): 'Suck',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Clean')): 'Left',\n",
    "             ((loc_D, 'Dirty'), (loc_D, 'Clean')): 'Left',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Dirty')): 'Suck',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Clean')): 'Up',\n",
    "             \n",
    "             ((loc_A, 'Dirty'), (loc_A, 'Clean'), (loc_C, 'Dirty')): 'Suck',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Dirty'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Clean'), (loc_D, 'Dirty')): 'Suck',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Clean'), (loc_D, 'Clean')): 'Left',\n",
    "             ((loc_B, 'Dirty'), (loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Dirty'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Clean'), (loc_C, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Clean'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_C, 'Dirty'), (loc_C, 'Clean'), (loc_D, 'Dirty')): 'Suck',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Dirty'), (loc_D, 'Clean')): 'Left',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Clean'), (loc_D, 'Dirty')): 'Suck',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Clean'), (loc_B, 'Clean')): 'Up',\n",
    "             ((loc_D, 'Dirty'), (loc_D, 'Clean'), (loc_B, 'Dirty')): 'Suck',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Dirty'), (loc_B, 'Clean')): 'Up',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Clean'), (loc_A, 'Clean')): 'Right',\n",
    "             \n",
    "             ((loc_A, 'Dirty'), (loc_A, 'Clean'), (loc_C, 'Dirty'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Dirty'), (loc_C, 'Clean'), (loc_D, 'Dirty')): 'Suck',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Dirty'), (loc_C, 'Clean'), (loc_D, 'Clean')): 'Left',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Clean'), (loc_D, 'Dirty'), (loc_D, 'Clean')): 'Left',\n",
    "             ((loc_A, 'Clean'), (loc_C, 'Clean'), (loc_D, 'Clean'), (loc_B, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Dirty'), (loc_B, 'Clean'), (loc_A, 'Dirty'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Dirty'), (loc_A, 'Clean'), (loc_C, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Dirty'), (loc_A, 'Clean'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Clean'), (loc_C, 'Dirty'), (loc_C, 'Clean')): 'Down',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Clean'), (loc_C, 'Clean'), (loc_D, 'Dirty')): 'Suck',\n",
    "             ((loc_C, 'Dirty'), (loc_C, 'Clean'), (loc_D, 'Dirty'), (loc_D, 'Clean')): 'Right',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Dirty'), (loc_D, 'Clean'), (loc_B, 'Dirty')): 'Suck',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Dirty'), (loc_D, 'Clean'), (loc_B, 'Clean')): 'Up',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Clean'), (loc_B, 'Dirty'), (loc_B, 'Clean')): 'Up',\n",
    "             ((loc_C, 'Clean'), (loc_D, 'Clean'), (loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck',\n",
    "             ((loc_D, 'Dirty'), (loc_D, 'Clean'), (loc_B, 'Dirty'), (loc_B, 'Clean')): 'Up',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Dirty'), (loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Dirty'), (loc_B, 'Clean'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Clean'), (loc_A, 'Dirty'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_D, 'Clean'), (loc_B, 'Clean'), (loc_A, 'Clean'), (loc_C, 'Dirty')): 'Suck'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a table-driven agent program for our two-state environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_A, loc_B, loc_C, loc_D = (0, 0), (1, 0), (0, 1), (1, 1)\n",
    "\n",
    "# Create a table-driven agent\n",
    "table_driven_agent = Agent(program=TableDrivenAgentProgram(table=table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the same environment, let's remove the previously added random agent from the environment to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_vacuum_env.delete_thing(random_agent)\n",
    "\n",
    "#Reset Environment\n",
    "trivial_vacuum_env.resetEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableDrivenVacuumAgent is located at (0, 0).\n"
     ]
    }
   ],
   "source": [
    "# Add the table-driven agent to the environment\n",
    "trivial_vacuum_env.add_thing(table_driven_agent)\n",
    "\n",
    "print(\"TableDrivenVacuumAgent is located at {}.\".format(table_driven_agent.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the Environment: {(0, 0): 'Clean', (1, 0): 'Dirty'}.\n",
      "TableDrivenVacuumAgent is located at (1, 0).\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # Run the environment\n",
    "    trivial_vacuum_env.step()\n",
    "\n",
    "    # Check the current state of the environment\n",
    "    print(\"State of the Environment: {}.\".format(trivial_vacuum_env.status))\n",
    "\n",
    "    print(\"TableDrivenVacuumAgent is located at {}.\".format(table_driven_agent.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE REFLEX AGENT PROGRAM\n",
    "\n",
    "A simple reflex agent program selects actions on the basis of the *current* percept, ignoring the rest of the percept history. These agents work on a **condition-action rule** (also called **situation-action rule**, **production** or **if-then rule**), which tells the agent the action to trigger when a particular situation is encountered.  \n",
    "\n",
    "The schematic diagram shown in **Figure 2.9** of the book will make this more clear:\n",
    "\n",
    "\"![simple reflex agent](images/simple_reflex_agent.jpg)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a simple reflex agent for the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the previously added table-driven agent\n",
    "trivial_vacuum_env.delete_thing(table_driven_agent)\n",
    "\n",
    "#Reset Environment\n",
    "trivial_vacuum_env.resetEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our agent, we need two functions: INTERPRET-INPUT function, which generates an abstracted description of the current state from the percerpt and the RULE-MATCH function, which returns the first rule in the set of rules that matches the given state description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_A = (0, 0)\n",
    "loc_B = (1, 0)\n",
    "loc_C = (0, 1)\n",
    "loc_D = (1, 1)\n",
    "        \n",
    "# Create a simple reflex agent the two-state environment\n",
    "simple_reflex_agent = ReflexVacuumAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the agent to the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleReflexVacuumAgent is located at (1, 0).\n"
     ]
    }
   ],
   "source": [
    "trivial_vacuum_env.add_thing(simple_reflex_agent)\n",
    "\n",
    "print(\"SimpleReflexVacuumAgent is located at {}.\".format(simple_reflex_agent.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the Environment: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "SimpleReflexVacuumAgent is located at (1, 0).\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # Run the environment\n",
    "    trivial_vacuum_env.step()\n",
    "    \n",
    "    # Check the current state of the environment\n",
    "    print(\"State of the Environment: {}.\".format(trivial_vacuum_env.status))\n",
    "    \n",
    "    print(\"SimpleReflexVacuumAgent is located at {}.\".format(simple_reflex_agent.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL-BASED REFLEX AGENT PROGRAM\n",
    "\n",
    "A model-based reflex agent maintains some sort of **internal state** that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. In addition to this, it also requires a **model** of the world, that is, knowledge about \"how the world works\".\n",
    "\n",
    "The schematic diagram shown in **Figure 2.11** of the book will make this more clear:\n",
    "<img src=\"files/images/model_based_reflex_agent.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a model-based reflex agent for the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the previously added simple reflex agent\n",
    "trivial_vacuum_env.delete_thing(simple_reflex_agent)\n",
    "\n",
    "#Reset Environment\n",
    "trivial_vacuum_env.resetEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need another function UPDATE-STATE which will be responsible for creating a new state description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelBasedVacuumAgent is located at (0, 0).\n"
     ]
    }
   ],
   "source": [
    "# Create a model-based reflex agent\n",
    "model_based_reflex_agent = ModelBasedVacuumAgent()\n",
    "\n",
    "# Add the agent to the environment\n",
    "trivial_vacuum_env.add_thing(model_based_reflex_agent)\n",
    "\n",
    "print(\"ModelBasedVacuumAgent is located at {}.\".format(model_based_reflex_agent.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the Environment: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "ModelBasedVacuumAgent is located at (1, 0).\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # Run the environment\n",
    "    trivial_vacuum_env.step()\n",
    "    \n",
    "    # Check the current state of the environment\n",
    "    print(\"State of the Environment: {}.\".format(trivial_vacuum_env.status))\n",
    "    \n",
    "    print(\"ModelBasedVacuumAgent is located at {}.\".format(model_based_reflex_agent.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOAL-BASED AGENT PROGRAM\n",
    "\n",
    "A goal-based agent needs some sort of **goal** information that describes situations that are desirable, apart from the current state description.\n",
    "\n",
    "**Figure 2.13** of the book shows a model-based, goal-based agent:\n",
    "<img src=\"files/images/model_goal_based_agent.jpg\">\n",
    "\n",
    "**Search** (Chapters 3 to 5) and **Planning** (Chapters 10 to 11) are the subfields of AI devoted to finding action sequences that achieve the agent's goals.\n",
    "\n",
    "## UTILITY-BASED AGENT PROGRAM\n",
    "\n",
    "A utility-based agent maximizes its **utility** using the agent's **utility function**, which is essentially an internalization of the agent's performance measure.\n",
    "\n",
    "**Figure 2.14** of the book shows a model-based, utility-based agent:\n",
    "<img src=\"files/images/model_utility_based_agent.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING AGENT\n",
    "\n",
    "Learning allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow. Here, we will breifly introduce the main ideas of learning agents.  \n",
    "\n",
    "A learning agent can be divided into four conceptual components. The **learning element** is responsible for making improvements. It uses the feedback from the **critic** on how the agent is doing and determines how the performance element should be modified to do better in the future. The **performance element** is responsible for selecting external actions for the agent: it takes in percepts and decides on actions. The critic tells the learning element how well the agent is doing with respect to a fixed performance standard. It is necesaary because the percepts themselves provide no indication of the agent's success. The last component of the learning agent is the **problem generator**. It is responsible for suggesting actions that will lead to new and informative experiences.  \n",
    "\n",
    "**Figure 2.15** of the book sums up the components and their working:  \n",
    "<img src=\"files/images/general_learning_agent.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
